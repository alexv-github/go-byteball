
// converted golang begin

import(
// _		"lodash"
		"async"
		"constants"
		"conf"
		"storage"
		"db"
 objectHash	"object_hash"
		"mutex"
		"main_chain"
 Definition	"definition"
 eventBus	"event_bus"
		"profiler"
)


count_writes := 0
count_units_in_prev_analyze := 0

func saveJoint_sync(objJoint objJointT, objValidationState objValidationStateT, preCommitCallback preCommitCallbackT) ErrorT {
	var(
		determineInputAddressFromSrcOutput_sync func (asset AssetT, denomination denominationT, input inputT) AddressT
		addInlinePaymentQueries_sync func () 
		updateBestParent_sync func () ErrorT
		determineMaxLevel_sync func () 
		updateLevel_sync func () ErrorT
		updateWitnessedLevel_sync func () ErrorT
		updateWitnessedLevelByWitnesslist_sync func (arrWitnesses AddressesT) ErrorT
	)
	
	objUnit := objJoint.unit
	console.log("saving unit " + objUnit.unit)
	profiler.start()
	conn := /* await */
	db.takeConnectionFromPool_sync()
	// << flattened continuation for db.takeConnectionFromPool:24:1
	arrQueries := AsyncFunctorsT{}
	conn.addQuery(arrQueries, "BEGIN")
	
	// additional queries generated by the validator, used only when received a doublespend
	for i := 0; i < len(objValidationState.arrAdditionalQueries); i++ {
		objAdditionalQuery := objValidationState.arrAdditionalQueries[i]
		console.log("----- applying additional queries: " + objAdditionalQuery.sql)
		conn.addQuery(arrQueries, objAdditionalQuery.sql, objAdditionalQuery.params)
	}
	
	fields := "unit, version, alt, witness_list_unit, last_ball_unit, headers_commission, payload_commission, sequence, content_hash"
	values := "?,?,?,?,?,?,?,?,?"
	params := {*ArrayExpression*}
	if conf.bLight {
		fields += ", main_chain_index, creation_date"
		values += ",?," + conn.getFromUnixTime("?")
		params = append(params, objUnit.main_chain_index)
	}
	conn.addQuery(arrQueries, "INSERT INTO units (" + fields + ") VALUES (" + values + ")", params)
	
	if objJoint.ball && ! conf.bLight {
		conn.addQuery(arrQueries, "INSERT INTO balls (ball, unit) VALUES(?,?)", DBParamsT{
			objJoint.ball,
			objUnit.unit,
		})
		conn.addQuery(arrQueries, "DELETE FROM hash_tree_balls WHERE ball=? AND unit=?", DBParamsT{
			objJoint.ball,
			objUnit.unit,
		})
		if objJoint.skiplist_units {
			for i := 0; i < len(objJoint.skiplist_units); i++ {
				conn.addQuery(arrQueries, "INSERT INTO skiplist_units (unit, skiplist_unit) VALUES (?,?)", DBParamsT{
					objUnit.unit,
					objJoint.skiplist_units[i],
				})
			}
		}
	}
	
	if objUnit.parent_units {
		for i := 0; i < len(objUnit.parent_units); i++ {
			conn.addQuery(arrQueries, "INSERT INTO parenthoods (child_unit, parent_unit) VALUES(?,?)", DBParamsT{
				objUnit.unit,
				objUnit.parent_units[i],
			})
		}
	}
	
	bGenesis := storage.isGenesisUnit(objUnit.unit)
	if bGenesis {
		conn.addQuery(arrQueries, "UPDATE units SET is_on_main_chain=1, main_chain_index=0, is_stable=1, level=0, witnessed_level=0 \n" +
			"				WHERE unit=?", DBParamsT{ objUnit.unit })
	} else {
		// .. not flattening for conn.addQuery
		conn.addQuery(arrQueries, "UPDATE units SET is_free=0 WHERE unit IN(?)", DBParamsT{ objUnit.parent_units }, func (result resultT) {*returns*} {
			// in sqlite3, result.affectedRows actually returns the number of _matched_ rows
			count_consumed_free_units := result.affectedRows
			console.log(count_consumed_free_units + " free units consumed")
			// .. not flattening for Array.forEach
			for parent_unit, _ := range objUnit.parent_units {
				if storage.assocUnstableUnits[parent_unit] {
					storage.assocUnstableUnits[parent_unit].is_free = 0
				}
			}
		})
	}
	
	if Array.isArray(objUnit.witnesses) {
		for i := 0; i < len(objUnit.witnesses); i++ {
			address := objUnit.witnesses[i]
			conn.addQuery(arrQueries, "INSERT INTO unit_witnesses (unit, address) VALUES(?,?)", DBParamsT{
				objUnit.unit,
				address,
			})
		}
		conn.addQuery(arrQueries, "INSERT " + conn.getIgnore() + " INTO witness_list_hashes (witness_list_unit, witness_list_hash) VALUES (?,?)", DBParamsT{
			objUnit.unit,
			objectHash.getBase64Hash(objUnit.witnesses),
		})
	}
	
	arrAuthorAddresses := AddressesT{}
	for i := 0; i < len(objUnit.authors); i++ {
		author := objUnit.authors[i]
		arrAuthorAddresses = append(arrAuthorAddresses, author.address)
		definition := author.definition
		definition_chash := nil
		if definition {
			// IGNORE for messages out of sequence
			definition_chash = objectHash.getChash160(definition)
			conn.addQuery(arrQueries, "INSERT " + conn.getIgnore() + " INTO definitions (definition_chash, definition, has_references) VALUES (?,?,?)", DBParamsT{
				definition_chash,
				JSON.stringify(definition),
				(Definition.hasReferences(definition) ? 1: 0),
			})
			// actually inserts only when the address is first used.
			// if we change keys and later send a unit signed by new keys, the address is not inserted. 
			// Its definition_chash was updated before when we posted change-definition message.
			if definition_chash == author.address {
				conn.addQuery(arrQueries, "INSERT " + conn.getIgnore() + " INTO addresses (address) VALUES(?)", DBParamsT{ author.address })
			}
		} else {
			if objUnit.content_hash {
				conn.addQuery(arrQueries, "INSERT " + conn.getIgnore() + " INTO addresses (address) VALUES(?)", DBParamsT{ author.address })
			}
		}
		conn.addQuery(arrQueries, "INSERT INTO unit_authors (unit, address, definition_chash) VALUES(?,?,?)", DBParamsT{
			objUnit.unit,
			author.address,
			definition_chash,
		})
		if bGenesis {
			conn.addQuery(arrQueries, "UPDATE unit_authors SET _mci=0 WHERE unit=?", DBParamsT{ objUnit.unit })
		}
		if ! objUnit.content_hash {
			for path := range author.authentifiers {
				conn.addQuery(arrQueries, "INSERT INTO authentifiers (unit, address, path, authentifier) VALUES(?,?,?,?)", DBParamsT{
					objUnit.unit,
					author.address,
					path,
					author.authentifiers[path],
				})
			}
		}
	}
	
	if ! objUnit.content_hash {
		for i := 0; i < len(objUnit.messages); i++ {
			message := objUnit.messages[i]
			
			text_payload := nil
			if message.app == "text" {
				text_payload = message.payload
			} else {
				if message.app == "data" || message.app == "profile" || message.app == "attestation" || message.app == "definition_template" {
					text_payload = JSON.stringify(message.payload)
				}
			}
			
			conn.addQuery(arrQueries, "INSERT INTO messages \n" +
				"					(unit, message_index, app, payload_hash, payload_location, payload, payload_uri, payload_uri_hash) VALUES(?,?,?,?,?,?,?,?)", DBParamsT{
				objUnit.unit,
				i,
				message.app,
				message.payload_hash,
				message.payload_location,
				text_payload,
				message.payload_uri,
				message.payload_uri_hash,
			})
			
			if message.payload_location == "inline" {
				[*SwitchStatement*]
			}
			// inline
			
			if "spend_proofs" in message {
				for j := 0; j < len(message.spend_proofs); j++ {
					objSpendProof := message.spend_proofs[j]
					conn.addQuery(arrQueries, "INSERT INTO spend_proofs (unit, message_index, spend_proof_index, spend_proof, address) VALUES(?,?,?,?,?)", DBParamsT{
						objUnit.unit,
						i,
						j,
						objSpendProof.spend_proof,
						objSpendProof.address || arrAuthorAddresses[0],
					})
				}
			}
		}
	}
	
	if "earned_headers_commission_recipients" in objUnit {
		for i := 0; i < len(objUnit.earned_headers_commission_recipients); i++ {
			recipient := objUnit.earned_headers_commission_recipients[i]
			conn.addQuery(arrQueries, "INSERT INTO earned_headers_commission_recipients (unit, address, earned_headers_commission_share) VALUES(?,?,?)", DBParamsT{
				objUnit.unit,
				recipient.address,
				recipient.earned_headers_commission_share,
			})
		}
	}
	
	my_best_parent_unit := {*init:null*}
	
	determineInputAddressFromSrcOutput_sync = func (asset AssetT, denomination denominationT, input inputT) AddressT {
		rows := /* await */
		conn.query_sync("SELECT address, denomination, asset FROM outputs WHERE unit=? AND message_index=? AND output_index=?", DBParamsT{
			input.unit,
			input.message_index,
			input.output_index,
		})
		// << flattened continuation for conn.query:237:3
		if len(rows) > 1 {
			_core.Throw("multiple src outputs found")
		}
		if len(rows) == 0 {
			if conf.bLight {
				// it's normal that a light client doesn't store the previous output
				// :: flattened return for return handleAddress(null);
				return nil
			} else {
				_core.Throw("src output not found")
			}
		}
		row := rows[0]
		if ! ! asset && ! row.asset || asset == row.asset {
			_core.Throw("asset doesn't match")
		}
		if denomination != row.denomination {
			_core.Throw("denomination doesn't match")
		}
		address := row.address
		if arrAuthorAddresses.indexOf(address) == - 1 {
			_core.Throw("src output address not among authors")
		}
		// :: flattened return for handleAddress(address);
		return address
		// >> flattened continuation for conn.query:237:3
	}
	
	addInlinePaymentQueries_sync = func ()  {
		// :: flattened return for cb(async.forEachOfSeries(objUnit.messages, function (message, i) {
		// ** need 0 return(s) instead of 1
		return (func () ErrorT {
		  // :: inlined async.eachOfSeries:263:3
		  for message, i := range objUnit.messages {
		    _err := (func (message messageT, i iT) ErrorT {
		    	if message.payload_location != "inline" {
		    		// :: flattened return for return cb2();
		    		// ** need 1 return(s) instead of 0
		    		return 
		    	}
		    	payload := message.payload
		    	if message.app != "payment" {
		    		// :: flattened return for return cb2();
		    		// ** need 1 return(s) instead of 0
		    		return 
		    	}
		    	
		    	denomination := payload.denomination || 1
		    	
		    	(func () ErrorT {
		    	  // :: inlined async.eachOfSeries:274:5
		    	  for input, j := range payload.inputs {
		    	    _err := (func (input inputT, j jT) ErrorT {
		    	    	var(
		    	    		determineInputAddress_sync func () AddressT
		    	    	)
		    	    	
		    	    	type := input.type || "transfer"
		    	    	src_unit := (type == "transfer" ? input.unit: nil)
		    	    	src_message_index := (type == "transfer" ? input.message_index: nil)
		    	    	src_output_index := (type == "transfer" ? input.output_index: nil)
		    	    	from_main_chain_index := (type == "witnessing" || type == "headers_commission" ? input.from_main_chain_index: nil)
		    	    	to_main_chain_index := (type == "witnessing" || type == "headers_commission" ? input.to_main_chain_index: nil)
		    	    	
		    	    	determineInputAddress_sync = func () AddressT {
		    	    		if type == "headers_commission" || type == "witnessing" || type == "issue" {
		    	    			// :: flattened return for return handleAddress(arrAuthorAddresses.length === 1 ? arrAuthorAddresses[0] : input.address);
		    	    			return (len(arrAuthorAddresses) == 1 ? arrAuthorAddresses[0]: input.address)
		    	    		}
		    	    		// hereafter, transfer
		    	    		if len(arrAuthorAddresses) == 1 {
		    	    			// :: flattened return for return handleAddress(arrAuthorAddresses[0]);
		    	    			return arrAuthorAddresses[0]
		    	    		}
		    	    		// :: flattened return for handleAddress(determineInputAddressFromSrcOutput(payload.asset, denomination, input));
		    	    		return /* await */
		    	    		determineInputAddressFromSrcOutput_sync(payload.asset, denomination, input)
		    	    	}
		    	    	address := /* await */
		    	    	determineInputAddress_sync()
		    	    	// << flattened continuation for determineInputAddress:293:7
		    	    	bDSIs := false
		    	    	for ds, _ := range objValidationState.arrDoubleSpendInputs {
		    	    		if ds.message_index == i && ds.input_index == j { bDSIs = true; break }
		    	    	}
		    	    	is_unique := 1
		    	    	if bDSIs {
		    	    		is_unique = nil
		    	    	}
		    	    	conn.addQuery(arrQueries, "INSERT INTO inputs \n" +
		    	    		"										(unit, message_index, input_index, type, \n" +
		    	    		"										src_unit, src_message_index, src_output_index, " +
		    	    		"										from_main_chain_index, to_main_chain_index, \n" +
		    	    		"										denomination, amount, serial_number, \n" +
		    	    		"										asset, is_unique, address) VALUES(?,?,?,?,?,?,?,?,?,?,?,?,?,?,?)", DBParamsT{
		    	    		objUnit.unit,
		    	    		i,
		    	    		j,
		    	    		type,
		    	    		src_unit,
		    	    		src_message_index,
		    	    		src_output_index,
		    	    		from_main_chain_index,
		    	    		to_main_chain_index,
		    	    		denomination,
		    	    		input.amount,
		    	    		input.serial_number,
		    	    		payload.asset,
		    	    		is_unique,
		    	    		address,
		    	    	})
		    	    	[*SwitchStatement*]
		    	    	// :: flattened return for cb3();
		    	    	// ** need 1 return(s) instead of 0
		    	    	return 
		    	    	// >> flattened continuation for determineInputAddress:293:7
		    	    })(input, j)
		    	    if _err != nil { return _err }
		    	  }
		    	  return nil
		    	})()
		    	// << flattened continuation for async.forEachOfSeries:274:5
		    	for j := 0; j < len(payload.outputs); j++ {
		    		output := payload.outputs[j]
		    		// we set is_serial=1 for public payments as we check that their inputs are stable and serial before spending, 
		    		// therefore it is impossible to have a nonserial in the middle of the chain (but possible for private payments)
		    		conn.addQuery(arrQueries, "INSERT INTO outputs \n" +
		    			"									(unit, message_index, output_index, address, amount, asset, denomination, is_serial) VALUES(?,?,?,?,?,?,?,1)", DBParamsT{
		    			objUnit.unit,
		    			i,
		    			j,
		    			output.address,
		    			parseInt(output.amount),
		    			payload.asset,
		    			denomination,
		    		})
		    	}
		    	// :: flattened return for cb2();
		    	// ** need 1 return(s) instead of 0
		    	return 
		    	// >> flattened continuation for async.forEachOfSeries:274:5
		    })(message, i)
		    if _err != nil { return _err }
		  }
		  return nil
		})()
	}
	
	updateBestParent_sync = func () ErrorT {
		rows := /* await */
		conn.query_sync("SELECT unit \n" +
			"				FROM units AS parent_units \n" +
			"				WHERE unit IN(?) \n" +
			"					AND (witness_list_unit=? OR ( \n" +
			"						SELECT COUNT(*) \n" +
			"						FROM unit_witnesses \n" +
			"						JOIN unit_witnesses AS parent_witnesses USING(address) \n" +
			"						WHERE parent_witnesses.unit IN(parent_units.unit, parent_units.witness_list_unit) \n" +
			"							AND unit_witnesses.unit IN(?, ?) \n" +
			"					)>=?) \n" +
			"				ORDER BY witnessed_level DESC, \n" +
			"					level-witnessed_level ASC, \n" +
			"					unit ASC \n" +
			"				LIMIT 1", DBParamsT{
			objUnit.parent_units,
			objUnit.witness_list_unit,
			objUnit.unit,
			objUnit.witness_list_unit,
			constants.COUNT_WITNESSES - constants.MAX_WITNESS_LIST_MUTATIONS,
		})
		// << flattened continuation for conn.query:346:3
		if len(rows) != 1 {
			_core.Throw("zero or more than one best parent unit?")
		}
		my_best_parent_unit = rows[0].unit
		if my_best_parent_unit != objValidationState.best_parent_unit {
			throwError("different best parents, validation: " + objValidationState.best_parent_unit + ", writer: " + my_best_parent_unit)
		}
		/* await */
		conn.query_sync("UPDATE units SET best_parent_unit=? WHERE unit=?", DBParamsT{
			my_best_parent_unit,
			objUnit.unit,
		})
		// << flattened continuation for conn.query:369:5
		// :: flattened return for cb();
		// ** need 1 return(s) instead of 0
		return 
		// >> flattened continuation for conn.query:369:5
		// >> flattened continuation for conn.query:346:3
	}
	
	determineMaxLevel_sync = func ()  {
		max_level := 0
		(func () ErrorT {
		  // :: inlined async.each:376:3 !! [tbd] finish this
		  for parent_unit := range objUnit.parent_units {
		    _err := (func (parent_unit UnitT) ErrorT {
		    	props := /* await */
		    	storage.readStaticUnitProps_sync(conn, parent_unit)
		    	// << flattened continuation for storage.readStaticUnitProps:379:5
		    	if props.level > max_level {
		    		max_level = props.level
		    	}
		    	// :: flattened return for cb();
		    	// ** need 1 return(s) instead of 0
		    	return 
		    	// >> flattened continuation for storage.readStaticUnitProps:379:5
		    })(parent_unit)
		    if _err != nil { return _err }
		  }
		  return nil
		})()
		// << flattened continuation for async.each:376:3
		// :: flattened return for handleMaxLevel(max_level);
		// ** need 0 return(s) instead of 1
		return max_level
		// >> flattened continuation for async.each:376:3
	}
	
	updateLevel_sync = func () ErrorT {
		rows := /* await */
		conn.query_sync("SELECT MAX(level) AS max_level FROM units WHERE unit IN(?)", DBParamsT{ objUnit.parent_units })
		// << flattened continuation for conn.query:392:3
		if len(rows) != 1 {
			_core.Throw("not a single max level?")
		}
		max_level := /* await */
		determineMaxLevel_sync()
		// << flattened continuation for determineMaxLevel:395:4
		if max_level != rows[0].max_level {
			throwError("different max level, sql: " + rows[0].max_level + ", props: " + max_level)
		}
		objNewUnitProps.level = max_level + 1
		/* await */
		conn.query_sync("UPDATE units SET level=? WHERE unit=?", DBParamsT{
			rows[0].max_level + 1,
			objUnit.unit,
		})
		// << flattened continuation for conn.query:399:5
		// :: flattened return for cb();
		// ** need 1 return(s) instead of 0
		return 
		// >> flattened continuation for conn.query:399:5
		// >> flattened continuation for determineMaxLevel:395:4
		// >> flattened continuation for conn.query:392:3
	}
	
	
	updateWitnessedLevel_sync = func () ErrorT {
		if objUnit.witnesses {
			// :: flattened return for cb(updateWitnessedLevelByWitnesslist(objUnit.witnesses));
			return /* await */
			updateWitnessedLevelByWitnesslist_sync(objUnit.witnesses)
		} else {
			arrWitnesses := /* await */
			storage.readWitnessList_sync(conn, objUnit.witness_list_unit)
			// << flattened continuation for storage.readWitnessList:411:4
			// :: flattened return for cb(updateWitnessedLevelByWitnesslist(arrWitnesses));
			return /* await */
			updateWitnessedLevelByWitnesslist_sync(arrWitnesses)
			// >> flattened continuation for storage.readWitnessList:411:4
		}
	}
	
	// The level at which we collect at least 7 distinct witnesses while walking up the main chain from our unit.
	// The unit itself is not counted even if it is authored by a witness
	updateWitnessedLevelByWitnesslist_sync = func (arrWitnesses AddressesT) ErrorT {
		var(
			setWitnessedLevel func (witnessed_level witnessed_levelT) 
			addWitnessesAndGoUp func (start_unit UnitT) (int, UnitT)
		)
		
		arrCollectedWitnesses := AddressesT{}
		
		setWitnessedLevel = func (witnessed_level witnessed_levelT)  {
			profiler.start()
			if witnessed_level != objValidationState.witnessed_level {
				throwError("different witnessed levels, validation: " + objValidationState.witnessed_level + ", writer: " + witnessed_level)
			}
			objNewUnitProps.witnessed_level = witnessed_level
			/* await */
			conn.query_sync("UPDATE units SET witnessed_level=? WHERE unit=?", DBParamsT{
				witnessed_level,
				objUnit.unit,
			})
			// << flattened continuation for conn.query:426:4
			profiler.stop("write-wl-update")
			// :: flattened return for cb();
			// ** need 1 return(s) instead of 0
			return 
			// >> flattened continuation for conn.query:426:4
		}
		
		addWitnessesAndGoUp = func (start_unit UnitT) (int, UnitT) {
			profiler.start()
			props := /* await */
			storage.readStaticUnitProps_sync(conn, start_unit)
			// << flattened continuation for storage.readStaticUnitProps:434:4
			profiler.stop("write-wl-select-bp")
			best_parent_unit := props.best_parent_unit
			level := props.level
			if level == nil {
				_core.Throw("null level in updateWitnessedLevel")
			}
			if level == 0 {
				// genesis
				setWitnessedLevel(0)
				return
			}
			profiler.start()
			arrAuthors := /* await */
			storage.readUnitAuthors_sync(conn, start_unit)
			// << flattened continuation for storage.readUnitAuthors:443:5
			profiler.stop("write-wl-select-authors")
			profiler.start()
			for i := 0; i < len(arrAuthors); i++ {
				address := arrAuthors[i]
				if arrWitnesses.indexOf(address) != - 1 && arrCollectedWitnesses.indexOf(address) == - 1 {
					arrCollectedWitnesses = append(arrCollectedWitnesses, address)
				}
			}
			profiler.stop("write-wl-search")
			if len(arrCollectedWitnesses) >= constants.MAJORITY_OF_WITNESSES {
				setWitnessedLevel(level)
				return
			}
			addWitnessesAndGoUp(best_parent_unit)
			// >> flattened continuation for storage.readUnitAuthors:443:5
			// >> flattened continuation for storage.readStaticUnitProps:434:4
		}
		
		profiler.stop("write-update")
		addWitnessesAndGoUp(my_best_parent_unit)
	}
	
	
	objNewUnitProps := [*ObjectExpression*]
	unlock := /* await */
	mutex.lock_sync({*ArrayExpression*})
	// << flattened continuation for mutex.lock:477:2
	console.log("got lock to write " + objUnit.unit)
	storage.assocUnstableUnits[objUnit.unit] = objNewUnitProps
	/* await */
	addInlinePaymentQueries_sync()
	// << flattened continuation for addInlinePaymentQueries:480:3
	(func () ErrorT {
	  // :: inlined async.series:481:4
	  for _f := range arrQueries {
	    if _err := _f() ; _err != nil { return _err }
	  }
	  return nil
	})()
	// << flattened continuation for async.series:481:4
	profiler.stop("write-raw")
	profiler.start()
	arrOps := AsyncFunctorsT{}
	if objUnit.parent_units {
		if ! conf.bLight {
			arrOps = append(arrOps, updateBestParent)
			arrOps = append(arrOps, updateLevel)
			arrOps = append(arrOps, updateWitnessedLevel)
			arrOps = append(arrOps, func () ErrorT {
				console.log("updating MC after adding " + objUnit.unit)
				/* await */
				main_chain.updateMainChain_sync(conn, nil, objUnit.unit)
				// << flattened continuation for main_chain.updateMainChain:492:8
				// :: flattened return for cb();
				// ** need 1 return(s) instead of 0
				return 
				// >> flattened continuation for main_chain.updateMainChain:492:8
			})
		}
		if preCommitCallback {
			arrOps = append(arrOps, func () ErrorT {
				console.log("executing pre-commit callback")
				/* await */
				preCommitCallback_sync(conn)
				// << flattened continuation for preCommitCallback:498:8
				// :: flattened return for cb();
				// ** need 1 return(s) instead of 0
				return 
				// >> flattened continuation for preCommitCallback:498:8
			})
		}
	}
	err := (func () ErrorT {
	  // :: inlined async.series:501:5
	  for _f := range arrOps {
	    if _err := _f() ; _err != nil { return _err }
	  }
	  return nil
	})()
	// << flattened continuation for async.series:501:5
	profiler.start()
	/* await */
	conn.query_sync((err ? "ROLLBACK": "COMMIT"))
	// << flattened continuation for conn.query:503:6
	conn.release()
	console.log((err ? err + ", therefore rolled back unit ": "committed unit ") + objUnit.unit)
	profiler.stop("write-commit")
	profiler.increment()
	if err {
		/* await */
		storage.resetUnstableUnits_sync()
		// << flattened continuation for storage.resetUnstableUnits:509:8
		unlock()
		// >> flattened continuation for storage.resetUnstableUnits:509:8
	} else {
		unlock()
	}
	if ! err {
		eventBus.emit("saved_unit-" + objUnit.unit, objJoint)
	}
	if onDone {
		// :: flattened return for onDone(err);
		return err
	}
	count_writes++
	if conf.storage == "sqlite" {
		updateSqliteStats()
	}
	// >> flattened continuation for conn.query:503:6
	// >> flattened continuation for async.series:501:5
	// >> flattened continuation for async.series:481:4
	// >> flattened continuation for addInlinePaymentQueries:480:3
	// >> flattened continuation for mutex.lock:477:2
	// >> flattened continuation for db.takeConnectionFromPool:24:1
}

func readCountOfAnalyzedUnits_sync()  {
	if count_units_in_prev_analyze {
		// :: flattened return for return handleCount(count_units_in_prev_analyze);
		// ** need 0 return(s) instead of 1
		return count_units_in_prev_analyze
	}
	rows := /* await */
	db.query_sync("SELECT * FROM sqlite_master WHERE type='table' AND name='sqlite_stat1'")
	// << flattened continuation for db.query:531:1
	if len(rows) == 0 {
		// :: flattened return for return handleCount(0);
		// ** need 0 return(s) instead of 1
		return 0
	}
	rows := /* await */
	db.query_sync("SELECT stat FROM sqlite_stat1 WHERE tbl='units' AND idx='sqlite_autoindex_units_1'")
	// << flattened continuation for db.query:534:2
	if len(rows) != 1 {
		console.log("no stat for sqlite_autoindex_units_1")
		// :: flattened return for return handleCount(0);
		// ** need 0 return(s) instead of 1
		return 0
	}
	// :: flattened return for handleCount(parseInt(rows[0].stat.split(' ')[0]));
	// ** need 0 return(s) instead of 1
	return parseInt(rows[0].stat.split(" ")[0])
	// >> flattened continuation for db.query:534:2
	// >> flattened continuation for db.query:531:1
}

start_time := 0
prev_time := 0
// update stats for query planner
func updateSqliteStats()  {
	if count_writes == 1 {
		start_time = Date.now()
		prev_time = Date.now()
	}
	if count_writes % 100 != 0 {
		return 
	}
	if count_writes % 1000 == 0 {
		total_time := Date.now() - start_time / 1000
		recent_time := Date.now() - prev_time / 1000
		recent_tps := 1000 / recent_time
		avg_tps := count_writes / total_time
		prev_time = Date.now()
	}
	rows := /* await */
	db.query_sync("SELECT MAX(rowid) AS count_units FROM units")
	// << flattened continuation for db.query:562:1
	count_units := rows[0].count_units
	if count_units > 500000 {
		// the db is too big
		return 
	}
	count_analyzed_units := /* await */
	readCountOfAnalyzedUnits_sync()
	// << flattened continuation for readCountOfAnalyzedUnits:566:2
	console.log("count analyzed units: " + count_analyzed_units)
	if count_units < 2 * count_analyzed_units {
		return 
	}
	count_units_in_prev_analyze = count_units
	console.log("will update sqlite stats")
	/* await */
	db.query_sync("ANALYZE")
	// << flattened continuation for db.query:572:3
	/* await */
	db.query_sync("ANALYZE sqlite_master")
	// << flattened continuation for db.query:573:4
	console.log("sqlite stats updated")
	// >> flattened continuation for db.query:573:4
	// >> flattened continuation for db.query:572:3
	// >> flattened continuation for readCountOfAnalyzedUnits:566:2
	// >> flattened continuation for db.query:562:1
}

func throwError(msg msgT)  {
	if typeof window == "undefined" {
		_core.Throw(msg)
	} else {
		eventBus.emit("nonfatal_error", msg, [*NewExpression*])
	}
}

exports.saveJoint = saveJoint


// converted golang end

